{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02392959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow-io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.io as tfio\n",
    "from tensorflow.io import gfile \n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a58c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming constants\n",
    "SOUNDS = \"Sound Recording\"\n",
    "#epsilon figures out the audio in the sample\n",
    "EPS = 0.1                   #0.1 is like the constant stuff yunooo\n",
    "REAL_LEN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5205d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shit ton of shit\n",
    "words = [\n",
    "    'backward',\n",
    "    'bed',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'dog',\n",
    "    'down',\n",
    "    'eight',\n",
    "    'five',\n",
    "    'follow',\n",
    "    'forward',\n",
    "    'four',\n",
    "    'go',\n",
    "    'happy',\n",
    "    'house',\n",
    "    'learn',\n",
    "    'left',\n",
    "    'apeke',\n",
    "    'nine',\n",
    "    'no',\n",
    "    'off',\n",
    "    'on',\n",
    "    'one',\n",
    "    'right',\n",
    "    'seven',\n",
    "    'six',\n",
    "    'stop',\n",
    "    'three',\n",
    "    'tree',\n",
    "    'two',\n",
    "    'up',\n",
    "    'yes',\n",
    "    'zero',\n",
    "    '_background',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d1c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the sound file and converts it to a list\n",
    "from glob import glob\n",
    "def get_file(word):\n",
    "    return gfile.glob(SOUNDS + '/' + word + '/*.wav')\n",
    "#trying to find where the audio begins and end\n",
    "#burnaboy & dave: Location of the audio\n",
    "def get_audio_position(audio, epsilon):\n",
    "    audio = audio - np.mean(audio)  #makes for a consistent audio\n",
    "    audio = audio /np.max(np.abs(audio))  #normalizing audio\n",
    "    return tfio.audio_trim(audio, axis = 0, epsilon = ESP)\n",
    "\n",
    "def get_lenght_of_audio(audio, epsilon):\n",
    "    position = get_lenght_of_audio(audio, epsilon)\n",
    "    lenght = (position[1] - position[0]) #figuring out the proper lenght of the audio\n",
    "    return lenght.numpy()\n",
    "\n",
    "#is there enough voice?\n",
    "def is_enough_voice(audio,epsilon, required_lenght):\n",
    "    voice_lenght = get_lenght_of_audio(au dio, epsilon)\n",
    "    return voice_lenght >= required_lenght           #returns a boolean\n",
    "\n",
    "#is the audio of appropriate size\n",
    "def appropriate_lenght(audio, expected_lenght):\n",
    "    return audio[0] == expected_lenght               #returns a boolean\n",
    "\n",
    "def valid_file(file_name):\n",
    "    #load file\n",
    "    audio_tensor = tfio.AudioIOTensor(file_name)\n",
    "    if not appropriate_lenght(audio_tensor, REAL_LEN):\n",
    "        return False\n",
    "    #normalizing once more form -1 to 1\n",
    "    audio = tf.cast(audio.tensor[:], tf.float32)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    #checking if there is voice in the audio\n",
    "    if not is_enough_voice(audio,epsilon, required_lenght):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7269ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(audio):\n",
    "    # normalizng the audio again\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # create the spectrogram\n",
    "    spectrogram = audio_ops.audio_spectrogram(audio,\n",
    "                                              window_size=320,  #values are subject to change\n",
    "                                              stride=160,\n",
    "                                              magnitude_squared=True).numpy()\n",
    "    # Average pooling\n",
    "    spectrogram = tf.nn.pool(\n",
    "        input=tf.expand_dims(spectrogram, -1), #adding a anew axis making it 3D\n",
    "        window_shape=[1, 6],\n",
    "        strides=[1, 6],\n",
    "        pooling_type='AVG', #specifying the pooling type\n",
    "        padding='SAME')     #ensuring the output is the same as the imput\n",
    "    spectrogram = tf.squeeze(spectrogram, axis=0)       #axis 0 removed the axis that was there before reducing it to a 2D shape\n",
    "    spectrogram = np.log10(spectrogram + 1e-6)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0119e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the file in the specrogram we mad above\n",
    "def processing_file(path):\n",
    "    # loading the audio data\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_path)\n",
    "    \n",
    "    # normalize the audio from values -1 and 1\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    # randomly reposition the audio in the sample\n",
    "    voice_begin, voice_end = get_voice_position(audio, ESP)\n",
    "    end_gap=len(audio) - voice_end\n",
    "    random_offset = np.random.uniform(0, voice_begin+end_gap)\n",
    "    audio = np.roll(audio,-random_offset+end_gap)      #i still dont properly understand this\n",
    "    \n",
    "    \n",
    "    #I will not introduce background noise in the beta testing of the model cause it will distort the data availbale.\n",
    "    #at this point im tired\n",
    "    # add some random background noise\n",
    "    background_volume = np.random.uniform(0, 0.1)      #ESP is 0.1\n",
    "    \n",
    "    # filtering in background noise files\n",
    "    background_noise = get_files('_background_noise_')\n",
    "    background_file = np.random.choice(background_noise)\n",
    "    background_tensor = tfio.audio.AudioIOTensor(background_file)\n",
    "    background_start = np.random.randint(0, (len(background_tensor) - REAL_LEN))  # I havent completely understood this\n",
    "    \n",
    "    # normalise the background noise\n",
    "    background = tf.cast(background_tensor[background_start:background_start+ REAL_LEN], tf.float32)\n",
    "    background = background - np.mean(background)\n",
    "    background = background / np.max(np.abs(background))\n",
    "    # mix the audio with the scaled background\n",
    "    audio = audio + background_volume * background\n",
    "    # get the spectrogram\n",
    "    return get_spectrogram(audio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b36444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data\n",
    "train = []\n",
    "validate = []\n",
    "test = []\n",
    "\n",
    "TRAIN_SIZE = 0.7\n",
    "VALIDATE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8297d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f351026c6d4279b4664b2a51bdf7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing words:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "#lots of refractoring to be done whalai\n",
    "def process_files(file_names, label, repeat=1):\n",
    "    file_names = tf.repeat(file_names, repeat).numpy()   #tqdm creates a progress bar\n",
    "    return [(process_file(file_names), label) for file_name in tqdm(file_names, desc=f\"{word} ({label})\", leave=False)]\n",
    "\n",
    "\n",
    "# process the files for a word into the spectrogram and one hot encoding word value\n",
    "def process_word(word, repeat=1):\n",
    "    # the index of the word word we are processing\n",
    "    label = words.index(word)\n",
    "    # get a list of files names for the word\n",
    "    file_names = [file_name for file_name in tqdm(get_files(word), desc=\"Checking\", leave=False) if is_valid_file(file_name)]\n",
    "    # randomly shuffle the filenames\n",
    "    np.random.shuffle(file_names)\n",
    "    # split the files into train, validate and test buckets\n",
    "    train_size=int(TRAIN_SIZE*len(file_names))\n",
    "    validation_size=int(VALIDATION_SIZE*len(file_names))\n",
    "    test_size=int(TEST_SIZE*len(file_names))\n",
    "    \n",
    "    \n",
    "    # get the training samples\n",
    "    train.extend(\n",
    "        process_files(\n",
    "            file_names[:train_size],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "    # and the validation samples\n",
    "    validate.extend(\n",
    "        process_files(\n",
    "            file_names[train_size:train_size+validation_size],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "    # and the test samples\n",
    "    test.extend(\n",
    "        process_files(\n",
    "            file_names[train_size+validation_size:],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )   #i dont know what i am doing \n",
    "\n",
    "# process all the words and all the files\n",
    "for word in tqdm(word, desc=\"Processing words\"):\n",
    "    if '_' not in word:\n",
    "        # add more examples of Apeke to balance our training set\n",
    "        repeat = 70 if word == 'apeke' else 1  #change word to KW which is apeke\n",
    "        #process_word(word, repeat=repeat)\n",
    "    \n",
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496eb911",
   "metadata": {},
   "source": [
    "*The block of code below hasnt been looked at because the initial testing of the code will require no background noise. Robustness of data isnt a cateria in our current build.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b71bf5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef95538224e84ed68d807efcd8b081b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Background Noise: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "#this block of code hasnt been looked at because the initial testing of the code will require no background noise.\n",
    "#robustness of data isnt a cateria in our current build\n",
    "# process the background noise files\n",
    "def process_background(file_name, label):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 8000), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "\n",
    "    # simulate random utterances\n",
    "    for section_index in tqdm(range(1000), desc=\"Simulated Words\", leave=False):\n",
    "        section_start = np.random.randint(0, audio_length - EXPECTED_SAMPLES)\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = np.reshape(audio[section_start:section_end], (EXPECTED_SAMPLES))\n",
    "\n",
    "        result = np.zeros((EXPECTED_SAMPLES))\n",
    "        # create a pseudo bit of voice\n",
    "        voice_length = np.random.randint(MINIMUM_VOICE_LENGTH/2, EXPECTED_SAMPLES)\n",
    "        voice_start = np.random.randint(0, EXPECTED_SAMPLES - voice_length)\n",
    "        hamming = np.hamming(voice_length)\n",
    "        # amplify the voice section\n",
    "        result[voice_start:voice_start+voice_length] = hamming * section[voice_start:voice_start+voice_length]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(np.reshape(section, (16000, 1)))\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "        \n",
    "for file_name in tqdm(get_file('_background_noise_'), desc=\"Processing Background Noise\"):\n",
    "    process_background(file_name, words.index(\"_background\"))\n",
    "    \n",
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b119e727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a78169feb145fd837edda739024752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing problem noise: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the data was given by that guy did dyi alexa\n",
    "#i still dont know if i should add this\n",
    "def process_problem_noise(file_name, label):\n",
    "    samples = []\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 400), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "\n",
    "for file_name in tqdm(get_file(\"_problem_noise_\"), desc=\"Processing problem noise\"):\n",
    "    process_problem_noise(file_name, words.index(\"_background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3290a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c06c393f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# randomise the training samples\u001b[39;00m\n\u001b[0;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(train)\n\u001b[1;32m----> 3\u001b[0m X_train, Y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtrain)\n\u001b[0;32m      4\u001b[0m X_validate, Y_validate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mvalidate)\n\u001b[0;32m      5\u001b[0m X_test, Y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtest)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# randomise the training samples\n",
    "np.random.shuffle(train)\n",
    "X_train, Y_train = zip(*train)\n",
    "X_validate, Y_validate = zip(*validate)\n",
    "X_test, Y_test = zip(*test)\n",
    "# save the computed data\n",
    "np.savez_compressed(\n",
    "    \"training_spectrogram.npz\",\n",
    "    X=X_train, Y=Y_train)\n",
    "print(\"Saved training data\")\n",
    "np.savez_compressed(\n",
    "    \"validation_spectrogram.npz\",\n",
    "    X=X_validate, Y=Y_validate)\n",
    "print(\"Saved validation data\")\n",
    "np.savez_compressed(\n",
    "    \"test_spectrogram.npz\",\n",
    "    X=X_test, Y=Y_test)\n",
    "print(\"Saved test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de95eb95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get the width and height of the spectrogram \"image\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m IMG_WIDTH\u001b[38;5;241m=\u001b[39m\u001b[43mX_train\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m IMG_HEIGHT\u001b[38;5;241m=\u001b[39mX_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_images2\u001b[39m(images_arr, imageWidth, imageHeight):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# get the width and height of the spectrogram \"image\"\n",
    "IMG_WIDTH=X_train[0].shape[0]\n",
    "IMG_HEIGHT=X_train[0].shape[1]\n",
    "def plot_images2(images_arr, imageWidth, imageHeight):\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(np.reshape(img, (imageWidth, imageHeight)))\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6be78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
